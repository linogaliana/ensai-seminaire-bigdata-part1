{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b935e9-1a72-405a-b240-00259f1274d5",
   "metadata": {},
   "source": [
    "# Démonstration Spark & Hive Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d6754b-21d0-4403-b4e4-e78bf9bf0ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b0302a-f109-44a1-b584-9b7de445c353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2544e9ba-170b-4756-bcc0-fadcea63d883",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 13:24:01,320 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-04-24 13:24:04,181 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "2023-04-24 13:24:04,660 WARN util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession \n",
    "         .builder\n",
    "         # configuration de la dynamicAllocation\n",
    "         # .config(\"spark.dynamicAllocation.enabled\",\"true\")\n",
    "         # .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n",
    "         # .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n",
    "         # .config(\"spark.dynamicAllocation.maxExecutors\",\"10\")\n",
    "         .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"20\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955d8da5-9146-48f7-92f1-2168881a5afc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                    READY   STATUS    RESTARTS   AGE\n",
      "pyspark-shell-5f97ba87b36f849a-exec-1   1/1     Running   0          6s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -l spark-role=executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5830182-3aaa-46d9-a39f-6d42c27bb3a0",
   "metadata": {},
   "source": [
    "## CSV to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8bdec-c3f4-4d6e-8933-c9c215618abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Partie illustrative, non reproductible__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ca9791-6def-4203-a71b-f9fd4d5f705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_residence;departement_residence;commune_residence;region_travail;departement_travail;commune_travail;commune_anterieure;commune_etude;pays_naissance;poids;sexe;statut_pro;densite;recherche_emploi;diplome;age;variable00;variable01;variable02;variable03;variable04;variable05;variable06;variable07;variable08;variable09;variable10;variable11;variable12;variable13;variable14;variable15;variable16;variable17;variable18;variable19;variable20;variable21;variable22;variable23;variable24;variable25;variable26;variable27;variable28;variable29;variable30;variable31;variable32;variable33;variable34;variable35;variable36;variable37;variable38;variable39;variable40;variable41;variable42;variable43;variable44;variable45;variable46;variable47;variable48;variable49\n",
      "53;22;22170;53;22;22170;22170;22170;99;1.197604824388689;2;Z;3;0;2;3;930660000002340;0001;006;9316;652242.5;6871102.2000000002;2;2017;01;1989;05;09;028027;027;027;2;75113;999;01;11;2;2;111;111;1;ZZZ;997;00;997;01;2;2;Z;YYYYY;ZZZZZ;2;93055;99999;16;16;1G;Z;1;00;930660902;1;93066;999;999;6\n"
     ]
    }
   ],
   "source": [
    "!mc head -n 2 s3/projet-onyxia/demo/rp/individus.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80381f3e-1116-4973-bdc4-a1f207949354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 09:18:56,481 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(spark.read\n",
    "      .format(\"csv\")\n",
    "      .options(header='true', inferschema='true', delimiter=';')\n",
    "      .load(\"s3a://projet-onyxia/demo/rp/individus.csv\")\n",
    "      .write\n",
    "      .mode('overwrite')\n",
    "      .parquet(\"s3a://projet-onyxia/demo/rp/individus.parquet\")\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "850aa7d8-f124-4d35-8002-83a226f40c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-04-28 09:21:44 UTC]     0B _SUCCESS\n",
      "[2022-04-28 09:21:11 UTC]  10MiB part-00000-033b363c-c084-4e78-8487-496b6d8e8bd3-c000.snappy.parquet\n",
      "[2022-04-28 09:21:23 UTC]  10MiB part-00001-033b363c-c084-4e78-8487-496b6d8e8bd3-c000.snappy.parquet\n",
      "[2022-04-28 09:20:47 UTC]  10MiB part-00002-033b363c-c084-4e78-8487-496b6d8e8bd3-c000.snappy.parquet\n",
      "[2022-04-28 09:21:35 UTC]  10MiB part-00003-033b363c-c084-4e78-8487-496b6d8e8bd3-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!mc ls s3/projet-onyxia/demo/rp/individus.parquet | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddaf91b-d6aa-4430-81d9-a1b4002c349c",
   "metadata": {},
   "source": [
    "## Requêtes SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc54288c-addb-4504-8fc2-45a366405ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 13:27:24,041 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df_parquet = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .options(header='true', inferschema='true', delimiter=';').\n",
    "    load(\"s3a://projet-formation/diffusion/bonnes-pratiques-r/rp_2016_individu_sample.csv\")\n",
    ")\n",
    "df_parquet.createOrReplaceTempView(\"individus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69423690-293c-4d08-9f4d-59b12b4a0b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+----+-----+----+------+------+-------+----+-----+----+-----+----+------+-----+----+----+----+-----+----+----+----+----+----+------+---+---+---+----+----+-----+-------+---+-----+----+----+---------+----+----+----+------+------+---+-----+----+----+----+-----+-----+------+----------------+----+------+-----+----+----+--------+----+----+----+----+-----+-----+-----+-----+----+------+------+-----+----+-----+----+-----+------+----+----+----+-------+----+---+----+---------+-----+----+----+-------+---+-----+----+-----+----+-----+-----+---+----+---+\n",
      "|region|  nummr|achlr|aemm|aemmr|aged|ager20|agerev|agerevq|anai|anarr|anem|anemr|apaf|arrivr|ascen|bain|bati|catl|catpc|chau|chfl|chos|clim|cmbl|couple|cs1|cs2|cs3|cuis|dept|derou|dipl_15|eau|egoul|elec|empl|     epci|etud|garl|hlml|iletud|iletuu|ilt|iltuu|immi|inai|inat|infam|inper|inperf|          ipondi|iran|iranuu|lienf|lprf|lprm|metrodom|moco|modv|na38|na88|naf08|naidt|nat13|nat49|natc|natn12|natn49|natnc|nbpi|nperr|numf|oridt|pnai12|prof|rech|sani|sanidom|sexe|sfm|stat|stat_conj|stocd|surf|tact|tactd16| tp|trans|typc|typfc|typl|typmd|typmr| ur|voit| wc|\n",
      "+------+-------+-----+----+-----+----+------+------+-------+----+-----+----+-----+----+------+-----+----+----+----+-----+----+----+----+----+----+------+---+---+---+----+----+-----+-------+---+-----+----+----+---------+----+----+----+------+------+---+-----+----+----+----+-----+-----+------+----------------+----+------+-----+----+----+--------+----+----+----+----+-----+-----+-----+-----+----+------+------+-----+----+-----+----+-----+------+----+----+----+-------+----+---+----+---------+-----+----+----+-------+---+-----+----+-----+----+-----+-----+---+----+---+\n",
      "|    52|263477 |    5|2000|    9|  19|    19|    18|     15|1998|    Z| 017|   03|   3|     Z|    2|   Z|   Z|   1|    0|   Z|   2|   Z|   Z|   6|     2|  6| 69| 69|   Z|99.0|    Z|      C|  Z|    Z|   Z|  15|999999999|   2|   1|   2|     Z|     Z|  2|    5|   2|   2|  11|    1|   5 |    5 |4.97145344686413|   1|     3|    3|   3|   3|       M|  11|  11|  AZ|  01|0141Z|    0|    0|  997|   0|    01|   997|    1|  05|    5|   1|    0|     1|691B|   Z|   2|     ZZ|   1| 33|  10|        B|   10|   6|  11|    111|  1|    4|   1|    2|   1|  303|   41|  1|   2|  Z|\n",
      "|    52|184369 |    5|2002|    9|  14|    14|    13|     10|2004|    Z| 016|   03|   3|     Z|    2|   Z|   Z|   1|    0|   Z|   3|   Z|   Z|   4|     2|  8| 82| 85|   Z|44.0|    Z|      Z|  Z|    Z|   Z|  ZZ|999999999|   1|   1|   2|     1|     3|  Z|    Z|   2|   1|  11|    1|   4 |    4 |4.83097484276848|   1|     3|    3|   3|   3|       M|  11|  11|  ZZ|  ZZ|ZZZZZ|    0|    0|  997|   0|    01|   997|    1|  04|    4|   1|    0|     1|ZZZZ|   Z|   2|     ZZ|   2| 32|  ZZ|        B|   10|   5|  23|    230|  Z|    Z|   1|    2|   1|  302|   41|  1|   2|  Z|\n",
      "+------+-------+-----+----+-----+----+------+------+-------+----+-----+----+-----+----+------+-----+----+----+----+-----+----+----+----+----+----+------+---+---+---+----+----+-----+-------+---+-----+----+----+---------+----+----+----+------+------+---+-----+----+----+----+-----+-----+------+----------------+----+------+-----+----+----+--------+----+----+----+----+-----+-----+-----+-----+----+------+------+-----+----+-----+----+-----+------+----+----+----+-------+----+---+----+---------+-----+----+----+-------+---+-----+----+-----+----+-----+-----+---+----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM individus\n",
    "WHERE region='52'\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f281a2-b765-4c14-a059-054408a311a1",
   "metadata": {},
   "source": [
    "## Partage des données via un Hive Metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef21b5-e9de-45f6-b94f-d992fe27e01d",
   "metadata": {},
   "source": [
    "__TO BE UPDATED__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f35ad9a-7e48-409a-b497-846622fe446c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 13:28:51,572 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |individus|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9172d3eb-717e-4bd2-8d54-5c0b99f14ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 11:44:06,870 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_str = ', '.join([' '.join(x) for x in df_parquet.dtypes])\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS individus_rp\n",
    "({schema_str})\n",
    "STORED as parquet LOCATION 'projet-onyxia/demo/rp/individus.parquet'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "954c206d-1941-465b-b506-d4bdafd8a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|individus_part_re...|      false|\n",
      "|  default|     individusregion|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070314b8-cf86-4f4f-8324-1e57763836de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+-------+----------+\n",
      "|        sum(poids)|sexe|densite|statut_pro|\n",
      "+------------------+----+-------+----------+\n",
      "| 84860.02165580631|   1|      4|         1|\n",
      "|  81647.7300888066|   1|      3|         1|\n",
      "| 20318.62707002139|   1|      3|         2|\n",
      "| 4594.674335848755|   2|      2|         2|\n",
      "|21197.941394322574|   2|      4|         2|\n",
      "| 81280.18280711232|   2|      3|         1|\n",
      "| 85804.86044169613|   2|      4|         1|\n",
      "| 2422.462061829637|   1|      1|         1|\n",
      "|20696.859938120786|   2|      3|         2|\n",
      "|21014.734028133804|   1|      4|         2|\n",
      "| 18598.93587046766|   1|      2|         1|\n",
      "|2403.5120489857827|   2|      1|         1|\n",
      "|18421.386742139224|   2|      2|         1|\n",
      "|4560.4938174609615|   1|      2|         2|\n",
      "| 607.7579233084527|   1|      1|         2|\n",
      "| 592.3437398684353|   2|      1|         2|\n",
      "+------------------+----+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_req1 = spark.sql(\"\"\"\n",
    "SELECT sum(poids), sexe, densite, statut_pro\n",
    "FROM individus_part_region_residence\n",
    "WHERE region_residence='44' AND region_travail != '44' AND statut_pro != 'Z'\n",
    "GROUP BY sexe, densite, statut_pro\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d313422f-0288-4fba-b054-1bd20a2b117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+-------+\n",
      "|        sum(poids)|age|diplome|\n",
      "+------------------+---+-------+\n",
      "| 9.177751090295155|  0|      1|\n",
      "| 6.360279128629814|  1|      6|\n",
      "|11.422012968672975|  1|      3|\n",
      "|14.023248192593126|  4|      7|\n",
      "|17.155410381367968|  2|      2|\n",
      "|10.180609173643692|  7|      1|\n",
      "|12.058477661296191|  1|      7|\n",
      "| 9.255402890330407|  3|      2|\n",
      "|11.077211876128839|  2|      1|\n",
      "| 7.591013552404388|  7|      0|\n",
      "|17.704675201373025|  0|      6|\n",
      "|11.098099052856005|  9|      1|\n",
      "|6.5043199507982346|  5|      0|\n",
      "|11.063367224180798|  7|      5|\n",
      "|10.859354234442353|  8|      6|\n",
      "| 8.077992082811662|  1|      1|\n",
      "|10.731451889825916|  5|      6|\n",
      "| 7.404974459515381|  9|      4|\n",
      "|10.519514716965539|  5|      3|\n",
      "|16.322972771017838|  5|      2|\n",
      "+------------------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_req2 = spark.sql(\"\"\"\n",
    "SELECT sum(poids), age, diplome\n",
    "FROM individus_part_region_residence\n",
    "WHERE recherche_emploi IN ('1','2') AND region_residence='11' AND commune_residence IN ('75013','75014','75015','92100','92130','92075','92240','92120','94250','94200')\n",
    "GROUP BY diplome, age\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e072c6ac-e021-4b06-b4fd-17b0adb0a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+---------------------+\n",
      "|        sum(poids)|pays_naissance|departement_residence|\n",
      "+------------------+--------------+---------------------+\n",
      "|  86885.8063479457|            51|                   52|\n",
      "| 74511.08026494365|            21|                   68|\n",
      "|101801.91621098612|            41|                   55|\n",
      "| 88498.68533646171|            12|                   10|\n",
      "| 86962.65446028857|            31|                   52|\n",
      "|148134.84723748424|            12|                   57|\n",
      "| 87066.47522814616|            21|                   52|\n",
      "|105343.26853180646|            31|                   67|\n",
      "| 74258.88213580017|            31|                   68|\n",
      "|103167.34376872572|            51|                   88|\n",
      "| 103272.6317022982|            31|                   88|\n",
      "|104006.69954490622|            21|                   88|\n",
      "|101665.45053942443|            10|                   55|\n",
      "|125388.10926533854|            12|                   51|\n",
      "| 92142.58091966441|            41|                   08|\n",
      "|124155.45150492407|            31|                   51|\n",
      "|120580.31747778569|            51|                   54|\n",
      "| 88264.35147426596|            21|                   10|\n",
      "|105381.27800799007|            41|                   67|\n",
      "| 91746.88160338369|            10|                   08|\n",
      "+------------------+--------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_req3 = spark.sql(\"\"\"\n",
    "SELECT sum(poids), pays_naissance, departement_residence\n",
    "FROM individus_part_region_residence\n",
    "WHERE region_residence='44'\n",
    "GROUP BY pays_naissance, departement_residence\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd4b45-274b-48c2-88c9-29e9025c9be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
